# Детали реализации

Техническая документация для разработчиков с описанием архитектуры, алгоритмов и внутренних механизмов системы.

## Содержание

1. [Архитектура системы](#архитектура-системы)
2. [Модули и их реализация](#модули-и-их-реализация)
3. [Алгоритмы и структуры данных](#алгоритмы-и-структуры-данных)
4. [Обработка ошибок](#обработка-ошибок)
5. [Зависимости и взаимодействия](#зависимости-и-взаимодействия)
6. [Расширяемость и рекомендации](#расширяемость-и-рекомендации)

---

## Архитектура системы

### Общая структура

Система состоит из четырех независимых модулей, работающих последовательно:

```
┌─────────────────────────┐
│ datasets_json_former.py │  → Анализ и индексация датасетов
└───────────┬─────────────┘
            │ Создает: datasets_info.json, class_names.json
            ↓
┌─────────────────────────┐
│   dataset_former.py      │  → Объединение и фильтрация
└───────────┬─────────────┘
            │ Использует: datasets_info.json, class_names.json
            │ Создает: объединенный датасет + data.yaml
            ↓
┌─────────────────────────┐
│ model_training_module.py│  → Обучение моделей YOLO
└───────────┬─────────────┘
            │ Использует: data.yaml
            │ Создает: обученные модели + метрики
            ↓
┌─────────────────────────┐
│   training_queue.py      │  → Управление очередью задач
└─────────────────────────┘
            │ Координирует выполнение всех модулей
```

### Принципы проектирования

1. **Модульность**: Каждый скрипт выполняет одну задачу и может работать независимо
2. **Конфигурация через JSON**: Метаданные хранятся в JSON для гибкости
3. **Обратная совместимость**: Поддержка различных форматов датасетов YOLO
4. **Обработка ошибок**: Graceful degradation с информативными сообщениями

---

## Модули и их реализация

### 1. datasets_json_former.py

#### Назначение
Сканирует директорию с датасетами и создает метаданные о структуре, классах и количестве элементов.

#### Алгоритм работы

```python
main()
├── Сканирование директории датасетов
├── Для каждого датасета:
│   ├── detect_structure() → определение структуры
│   ├── process_dataset() → извлечение информации
│   │   ├── find_yaml_file() или find_obj_names_file()
│   │   ├── load_yaml() или load_obj_names()
│   │   └── count_elements()
│   └── Добавление в datasets_info
└── Сохранение JSON файлов
```

#### Ключевые функции

**`detect_structure(folder_path)`**
- **Алгоритм**: Иерархическая проверка структуры папок
- **Порядок проверки**:
  1. Darknet: проверка `obj_train_data` + (`obj.names` или `obj.data`)
  2. Split: наличие папок `train`, `val`, `test` на верхнем уровне
  3. Nested Split: наличие `images/train`, `images/val`, `images/test`
  4. Flat: наличие только `images` и `labels`
  5. Unknown: если ничего не подошло
- **Сложность**: O(n), где n - количество подпапок

**`count_elements(folder_path, structure)`**
- **Реализация**: Рекурсивный обход файловой системы
- **Оптимизация**: Использует `os.listdir()` вместо `os.walk()` для плоских структур
- **Валидация**: Проверяет соответствие количества изображений и аннотаций
- **Возвращает**: 
  - `int` если количества совпадают
  - `tuple(int, int)` если не совпадают (для предупреждения)

**`process_dataset(folder_path, folder_name)`**
- **Стратегия загрузки классов**:
  1. Приоритет: YAML файл (формат YOLOv8/YOLOv11)
  2. Fallback: Darknet формат (`obj.names`)
  3. Обработка разных форматов `names`:
     - Список: `['class1', 'class2']`
     - Словарь: `{0: 'class1', 1: 'class2'}` → сортировка по ключам
- **Обработка ошибок**: Возвращает `None` при неудаче, пропускает датасет

#### Структура данных

**Входные данные**:
- Директория с датасетами
- Каждый датасет в отдельной подпапке

**Выходные данные**:
```python
# datasets_info.json
{
    "dataset_name": {
        "classes": {str: int},      # имя класса → индекс
        "structure": str,            # "split" | "flat" | "nested_split" | "darknet"
        "elements_count": int | list # количество или [train, val, test]
    }
}

# class_names.json
{
    "original_name": "normalized_name"  # маппинг для нормализации
}
```

#### Особенности реализации

1. **Кодировка UTF-8**: Все файлы читаются с явным указанием кодировки
2. **Case-insensitive поиск**: Поиск файлов `data.yaml`/`data.yml` без учета регистра
3. **Рекурсивный поиск**: `os.walk()` для поиска файлов в подпапках
4. **Валидация данных**: Проверка типов и форматов перед сохранением

---

### 2. dataset_former.py

#### Назначение
Объединяет несколько датасетов в один, фильтруя по выбранным классам и переиндексируя их.

#### Алгоритм работы

```python
main()
├── Загрузка datasets_info.json и class_names.json
├── Поиск подходящих датасетов (содержат все выбранные классы)
├── Подсчет общего количества файлов для прогресс-бара
├── Для каждого датасета:
│   ├── find_dataset_paths() → получение путей к изображениям/аннотациям
│   ├── Создание пар (изображение, аннотация)
│   ├── Перемешивание пар
│   ├── Разделение на train/val/test (80/10/10)
│   └── Для каждой пары:
│       ├── filter_label_file() → фильтрация и переиндексация
│       └── Копирование изображения (если аннотация не пустая)
└── Создание data.yaml
```

#### Ключевые функции

**`find_dataset_paths(dataset_path, structure, arg=False)`**
- **Параметр `arg`**: Если `True`, исключает `test` из поиска (для `--exclude-test`)
- **Логика для каждого типа структуры**:
  - **Split**: `{split}/images`, `{split}/labels` для каждого split
  - **Flat**: `images`, `labels` на верхнем уровне
  - **Nested Split**: `images/{split}`, `labels/{split}`
  - **Darknet**: `obj_train_data` (изображения и аннотации в одной папке)
- **Возвращает**: Список кортежей `(images_path, labels_path)`

**`filter_label_file(src_label_path, dst_label_path, class_map, class_names_map, selected_classes)`**
- **Алгоритм фильтрации**:
  1. Создание маппинга `class_id → normalized_name` из `class_map` и `class_names_map`
  2. Создание нового маппинга `normalized_name → new_id` для выбранных классов
  3. Парсинг каждой строки аннотации:
     - Извлечение `class_id`
     - Нормализация через `class_names_map`
     - Проверка наличия в `selected_classes`
     - Переиндексация на новый ID
  4. Сохранение только отфильтрованных строк
- **Возвращает**: `True` если файл содержит выбранные классы, иначе `False`
- **Особенности**:
  - Пропускает пустые строки
  - Обрабатывает ошибки парсинга (некорректные строки)
  - Сохраняет координаты bounding box без изменений

**Разделение на train/val/test**
- **Алгоритм**: Простое разделение массива по индексам
- **Пропорции**: `TRAIN_PART = 0.8`, `VAL_PART = 0.1`, `TEST_PART = 0.1`
- **Реализация**:
  ```python
  train_split = pairs[:int(n * TRAIN_PART)]
  val_split = pairs[int(n * TRAIN_PART):int(n * (TRAIN_PART + VAL_PART))]
  test_split = pairs[int(n * (VAL_PART + TRAIN_PART)):]
  ```
- **Особенность**: Разделение происходит для каждого датасета отдельно, затем объединение

#### Структура данных

**Внутренние структуры**:
```python
# Маппинг классов исходного датасета
class_map = {"helmet": 0, "gloves": 1, "vest": 2}

# Маппинг нормализации
class_names_map = {"Helmet": "helmet", "helmet": "helmet"}

# Выбранные классы
selected_classes = ["helmet", "gloves"]

# Новый маппинг для выходного датасета
new_id_map = {"helmet": 0, "gloves": 1}
```

**Пара данных**:
```python
pair = (image_path: str, label_path: str)
```

#### Особенности реализации

1. **Именование файлов**: Префикс с именем исходного датасета + глобальный счетчик
   - Формат: `{dataset_name}_{counter}.{ext}`
   - Гарантирует уникальность имен

2. **Копирование файлов**: Использует `shutil.copy2()` для сохранения метаданных

3. **Прогресс-бар**: Использует `tqdm` для визуализации прогресса
   - Обновляется для каждого обработанного файла
   - Показывает общий прогресс по всем датасетам

4. **Фильтрация**: Изображение копируется только если аннотация содержит выбранные классы
   - Экономит место на диске
   - Упрощает последующую обработку

5. **Создание data.yaml**: Простое текстовое создание файла
   - Относительные пути для портативности
   - Формат совместим с Ultralytics YOLO

---

### 3. model_training_module.py

#### Назначение
Обучение и тестирование моделей YOLO с использованием библиотеки Ultralytics.

#### Алгоритм работы

```python
main()
├── Парсинг аргументов командной строки
├── Если не --test-only:
│   ├── train_yolo()
│   │   ├── Валидация датасета и data.yaml
│   │   ├── Создание структуры папок для результатов
│   │   ├── Проверка существования (запрос подтверждения)
│   │   ├── Загрузка модели YOLO
│   │   └── model.train() → обучение
│   └── test_yolo() → автоматическое тестирование
└── Если --test-only:
    └── test_yolo() → только тестирование
```

#### Ключевые функции

**`train_yolo(dataset_path, model_version, epochs, batch, img_size, target_dir)`**
- **Валидация входных данных**:
  - Проверка существования директории датасета
  - Проверка наличия `data.yaml`
- **Структура выходных папок**:
  ```
  target_dir/
  └── dataset_name/
      └── {YYYY-MM-DD_HH-MM}_{model}_{epochs}epochs-{hash}/
          ├── train/
          │   └── weights/
          │       ├── best.pt
          │       └── last.pt
          ├── test_metrics.csv
          └── training_metadata.json
  ```
- **Вычисление хеша датасета**:
  - Перед созданием директории вычисляется хеш датасета с помощью `calculate_dataset_hash()` из `dataset_hash.py`
  - Хеш основан на структуре папок, именах файлов и их размерах
  - Не зависит от даты/времени изменения файлов
  - Возвращает первые 8 символов MD5 хеша (hex)
  - Если вычисление хеша не удалось, хеш не добавляется в имя папки
- **Именование директорий**:
  - Формат: `{YYYY-MM-DD_HH-MM}_{model}_{epochs}epochs-{hash}`
  - Пример: `2025-12-06_15-10_yolo11n_10epochs-a1b2c3d4`
  - Дата и время берутся в момент начала обучения
  - Хеш датасета добавляется в конец имени для уникальной идентификации датасета
  - Гарантирует уникальность и хронологическую сортировку
- **Обработка версий моделей**:
  - Если указано без расширения: добавляет `.pt`
  - Поддерживает форматы: `yolov8n`, `yolov8n.pt`
- **Отслеживание времени**:
  - Фиксирует `training_start_time` перед созданием директории
  - Фиксирует `training_end_time` после завершения обучения
- **Интеграция с Ultralytics**:
  ```python
  model.train(
      data=data_yaml,      # путь к data.yaml
      epochs=epochs,       # количество эпох
      batch=batch,         # размер batch
      imgsz=img_size,      # размер изображения
      project=model_dir,   # базовая директория
      name="train",        # имя подпапки
      exist_ok=False       # не перезаписывать существующие
  )
  ```

**`test_yolo(model_dir, dataset_path)`**
- **Загрузка модели**: Из `model_dir/train/weights/best.pt`
- **Валидация**: Использует тестовый набор из `data.yaml`
- **Сохранение метрик**: Автоматическое сохранение в CSV
- **Структура результатов**:
  ```
  model_dir/
  ├── train/...
  └── test/
      └── ... (результаты валидации)
  └── test_metrics.csv
  ```

**`save_metrics_csv(test_result, model_dir)`**
- **Обработка конфликтов имен**: Добавляет суффикс `_1`, `_2`, ... если файл существует
- **Формат данных**: Использует встроенный метод `to_csv()` объекта результата Ultralytics
- **Кодировка**: UTF-8 для корректной работы с русскими символами

**`save_training_metadata(model_dir, dataset_path, ..., dataset_hash=None)`**
- **Назначение**: Сохраняет полные метаданные эксперимента в JSON формате
- **Расположение**: Файл `training_metadata.json` создается в директории модели рядом с `test_metrics.csv`
- **Содержимое**:
  - Информация о модели, датасете, гиперпараметрах
  - Хеш датасета (8 символов hex) в секции `training_info.dataset.hash`
  - Временные метки начала и окончания обучения/тестирования
  - Статус успешности выполнения (success/error)
  - Относительные пути для переносимости
- **Обработка ошибок**: Сохраняет полный traceback при возникновении ошибок
- **Подробнее**: См. [training_metadata.md](training_metadata.md)

**`_get_relative_path(target_path, base_path)`**
- **Назначение**: Вычисляет относительный путь от директории модели к датасету
- **Алгоритм**: Использует `os.path.relpath()` с обработкой граничных случаев
- **Fallback**: Возвращает абсолютный путь если пути на разных дисках

#### Структура данных

**Конфигурация обучения**:
```python
config = {
    "data": str,        # путь к data.yaml
    "epochs": int,      # количество эпох
    "batch": int,       # размер batch
    "imgsz": int,       # размер изображения (обычно 640)
    "project": str,     # базовая директория
    "name": str         # имя подпапки
}
```

**Результат тестирования**:
- Объект `Results` от Ultralytics
- Содержит метрики: mAP, precision, recall, F1-score
- Экспортируется в CSV через `to_csv()`

#### Особенности реализации

1. **Обработка путей**: Использует `os.path.normpath()` для нормализации путей
2. **Извлечение имени датасета**: `os.path.basename()` для получения имени из пути
3. **Обработка расширений**: Парсинг через `os.path.splitext()`
4. **Интерактивный режим**: Использует `input()` для подтверждения действий
5. **Обработка исключений**: 
   - Обучение и тестирование обернуты в `try-except` блоки
   - При ошибке сохраняется полный traceback в метаданных
   - Директория для метаданных создается даже при ошибке обучения
   - Статус ошибки записывается в секцию `status` файла метаданных
6. **Именование директорий**:
   - Формат: `{YYYY-MM-DD_HH-MM}_{model}_{epochs}epochs-{hash}`
   - Дата и время берутся в момент начала обучения
   - Хеш датасета добавляется в конец имени для уникальной идентификации
   - Гарантирует уникальность и хронологическую сортировку
7. **Вычисление хеша датасета**:
   - Используется функция `calculate_dataset_hash()` из модуля `dataset_hash.py`
   - Хеш вычисляется на основе структуры папок, имен файлов и их размеров
   - Не зависит от даты/времени изменения файлов
   - Возвращает первые 8 символов MD5 хеша
   - При ошибке вычисления хеш не добавляется в имя папки и метаданные
8. **Сохранение метаданных**:
   - Автоматически вызывается после завершения обучения и тестирования
   - Содержит полную информацию об эксперименте для воспроизводимости
   - Включает хеш датасета в секции `training_info.dataset.hash`
   - Использует относительные пути для переносимости между системами

---

### 4. dataset_hash.py

#### Назначение
Вычисление хеша датасета на основе структуры папок, имен файлов и их размеров для уникальной идентификации датасета.

#### Алгоритм работы

```python
calculate_dataset_hash(dataset_path)
├── Валидация существования и типа пути
├── Рекурсивный обход всех папок и файлов (os.walk)
├── Для каждого элемента:
│   ├── Сортировка для детерминированности
│   ├── Для папки: добавление типа и относительного пути в хеш
│   └── Для файла: добавление типа, относительного пути и размера в хеш
├── Вычисление MD5 хеша собранной информации
└── Возврат первых 8 символов hex
```

#### Ключевые функции

**`calculate_dataset_hash(dataset_path)`**
- **Алгоритм**: Рекурсивный обход файловой системы с использованием `os.walk()`
- **Учитывает**:
  - Структуру папок (имена и относительные пути)
  - Имена файлов (относительные пути)
  - Размеры файлов (`os.path.getsize()`)
- **Игнорирует**:
  - Дату/время изменения файлов
  - Служебные файлы (`.DS_Store`, `Thumbs.db`, `.gitkeep`, `.gitignore`)
- **Детерминированность**: Сортировка всех элементов перед хешированием
- **Кодировка**: UTF-8 для корректной работы с русскими именами файлов
- **Возвращает**: Первые 8 символов MD5 хеша (hex), например `a1b2c3d4`
- **Обработка ошибок**: Вызывает исключения при отсутствии пути или ошибках доступа

**`main()`**
- **Аргументы командной строки**:
  - `dataset_path` (обязательный): Путь к папке с датасетом
  - `--validate HASH` (опциональный): Ожидаемое значение хеша для валидации
- **Режим вычисления**: Выводит хеш на stdout, возвращает код 0
- **Режим валидации**: Сравнивает вычисленный хеш с ожидаемым
  - Возвращает код 1 при успешной валидации
  - Возвращает код 0 при ошибке валидации
- **Использование**:
  ```bash
  # Вычисление хеша
  python3 dataset_hash.py /path/to/dataset
  
  # Валидация хеша
  python3 dataset_hash.py /path/to/dataset --validate a1b2c3d4
  ```

#### Структура данных

**Входные данные**:
- Путь к директории датасета (любой формат: split, flat, nested_split, darknet)

**Выходные данные**:
- Строка из 8 символов hex (MD5 хеш)

#### Особенности реализации

1. **Производительность**: Использует только метаданные файлов (`os.path.getsize()`), не читает содержимое
2. **Детерминированность**: Сортировка всех элементов гарантирует одинаковый хеш для одинаковых датасетов
3. **Кроссплатформенность**: Работает на всех платформах, поддерживающих Python
4. **Обработка ошибок**: Информативные сообщения об ошибках через stderr
5. **Валидация**: Поддержка проверки хеша через аргумент командной строки

---

### 5. training_queue.py

#### Назначение
Система очереди для последовательного выполнения задач обучения.

#### Алгоритм работы

```python
main()
├── Открытие окна мониторинга статуса
├── Загрузка существующих статусов
└── Бесконечный цикл:
    ├── Чтение training_queue.txt
    ├── Добавление новых задач в статусы
    ├── Поиск следующей задачи со статусом "Ждет выполнения"
    ├── Если найдена:
    │   ├── Установка статуса "Выполняется"
    │   ├── Обработка команды (process_line)
    │   ├── Запуск процесса (start_new_process)
    │   ├── Ожидание завершения
    │   └── Обновление статуса ("Выполнено" или "Ошибка")
    └── Если не найдена: ожидание 5 секунд
```

#### Ключевые функции

**`main_window()`**
- **Реализация**: Использует `subprocess.Popen()` для запуска отдельного терминала
- **Команда**: `watch -n 1 cat status.txt`
- **Особенность**: Работает только в GNOME (Linux)
- **Альтернатива**: Можно заменить на другие терминальные эмуляторы

**`process_line(line)`**
- **Алгоритм обработки**:
  1. Разделение строки на аргументы
  2. Пропуск комментариев (строки начинающиеся с `#`)
  3. Пропуск пустых строк
  4. Добавление `python3` если отсутствует
  5. Добавление `.py` если отсутствует расширение
- **Возвращает**: Обработанную команду или `None`

**`start_new_process(cmd)`**
- **Реализация**: `subprocess.Popen()` с `shell=True`
- **Ожидание**: `process.wait()` блокирует выполнение до завершения
- **Возвращает**: Код возврата процесса (0 = успех)

**`load_statuses()` / `save_statuses(statuses)`**
- **Формат файла**: `задача | статус\n`
- **Парсинг**: Разделение по ` | ` (пробел-вертикальная черта-пробел)
- **Структура данных**: `dict[str, str]` - задача → статус

#### Структура данных

**Статусы задач**:
```python
statuses = {
    "dataset_former.py --classes helmet": "Ждет выполнения",
    "model_training_module.py --model yolov8n": "Выполняется",
    ...
}
```

**Формат файла статуса**:
```
dataset_former.py --classes helmet | Ждет выполнения
model_training_module.py --model yolov8n | Выполняется
```

#### Особенности реализации

1. **Атомарность операций**: Чтение-модификация-запись статусов
2. **Обработка прерываний**: `finally` блок для очистки файла статуса
3. **Проверка существования**: Проверка файла статуса перед чтением
4. **Инкрементальное обновление**: Добавление только новых задач в статусы
5. **Последовательное выполнение**: Одна задача за раз, строгий порядок

#### Ограничения

1. **Платформо-зависимость**: Требует GNOME терминал
2. **Нет параллелизма**: Задачи выполняются строго последовательно
3. **Нет приоритетов**: Порядок выполнения соответствует порядку в файле
4. **Нет перезапуска**: При ошибке задача помечается как "Ошибка" и не перезапускается

---

## Алгоритмы и структуры данных

### Алгоритм определения структуры датасета

```python
def detect_structure(folder_path):
    # Приоритет 1: Darknet формат
    if exists("obj_train_data") and (exists("obj.names") or exists("obj.data")):
        return "darknet"
    
    # Приоритет 2: Split формат
    subfolders = listdir(folder_path)
    if any(x in subfolders for x in ["train", "val", "test"]):
        return "split"
    
    # Приоритет 3: Nested Split
    if exists("images") and exists("labels"):
        images_sub = listdir("images")
        if any(x in images_sub for x in ["train", "val", "test"]):
            return "nested_split"
        return "flat"
    
    return "unknown"
```

**Сложность**: O(n), где n - количество подпапок

### Алгоритм фильтрации и переиндексации классов

```python
def filter_label_file(src, dst, class_map, class_names_map, selected_classes):
    # Шаг 1: Создание маппинга class_id → normalized_name
    id_to_normalized = {}
    for name, idx in class_map.items():
        normalized = class_names_map.get(name, name)
        id_to_normalized[idx] = normalized
    
    # Шаг 2: Создание нового маппинга normalized_name → new_id
    new_id_map = {cls: i for i, cls in enumerate(selected_classes)}
    
    # Шаг 3: Фильтрация и переиндексация
    filtered_lines = []
    for line in readlines(src):
        class_id = int(line.split()[0])
        normalized_name = id_to_normalized[class_id]
        
        if normalized_name in selected_classes:
            new_id = new_id_map[normalized_name]
            # Замена class_id на new_id
            filtered_lines.append(line.replace(class_id, new_id))
    
    # Шаг 4: Сохранение
    writelines(dst, filtered_lines)
```

**Сложность**: O(m × k), где m - количество строк в файле, k - длина строки

### Алгоритм разделения на train/val/test

```python
def split_dataset(pairs, train_part=0.8, val_part=0.1, test_part=0.1):
    random.shuffle(pairs)  # Перемешивание
    n = len(pairs)
    
    train_end = int(n * train_part)
    val_end = int(n * (train_part + val_part))
    
    train = pairs[:train_end]
    val = pairs[train_end:val_end]
    test = pairs[val_end:]
    
    return train, val, test
```

**Особенности**:
- Использует `random.shuffle()` для перемешивания
- Seed задается глобально: `random.seed(12345)`
- Гарантирует воспроизводимость результатов

---

## Обработка ошибок

### Стратегии обработки ошибок

1. **Graceful degradation**: При ошибке обработки одного датасета система продолжает работу с остальными
2. **Информативные сообщения**: Все ошибки выводятся с префиксом `[ERROR]` или `[WARNING]`
3. **Валидация входных данных**: Проверка существования файлов и корректности форматов
4. **Возврат значений**: Функции возвращают `None` при ошибке вместо исключений

### Типичные ошибки и их обработка

**datasets_json_former.py**:
- Файл не найден → `[WARNING]`, пропуск датасета
- Некорректный формат YAML → `[ERROR]`, пропуск датасета
- Ошибка записи JSON → `[ERROR]`, завершение с сообщением

**dataset_former.py**:
- Датасет не содержит классы → `[ERROR]`, завершение
- Ошибка чтения файла → Пропуск файла, продолжение
- Ошибка копирования → Пропуск файла, продолжение

**model_training_module.py**:
- Датасет не найден → `FileNotFoundError` с описанием
- Модель не найдена → Исключение от Ultralytics
- Ошибка обучения → `[ERROR]` с описанием, возврат `model_dir`

**training_queue.py**:
- Файл очереди не найден → Пустой список, ожидание
- Ошибка выполнения команды → Статус "Ошибка", продолжение
- Ошибка чтения статуса → Пустой словарь, создание нового

---

## Зависимости и взаимодействия

### Внешние зависимости

**datasets_json_former.py**:
- `yaml` (PyYAML) - парсинг YAML файлов
- `json` (стандартная библиотека) - работа с JSON
- `os`, `sys` (стандартная библиотека)

**dataset_former.py**:
- `tqdm` - прогресс-бар
- `shutil` (стандартная библиотека) - копирование файлов
- `random` (стандартная библиотека) - перемешивание
- `json`, `os` (стандартная библиотека)

**model_training_module.py**:
- `ultralytics` - работа с YOLO моделями
- `dataset_hash` - вычисление хеша датасета
- `os`, `sys`, `argparse` (стандартная библиотека)

**dataset_hash.py**:
- `hashlib` (стандартная библиотека) - вычисление MD5 хеша
- `os`, `sys`, `argparse` (стандартная библиотека)

**training_queue.py**:
- `subprocess` (стандартная библиотека) - запуск процессов
- `os`, `sys`, `time` (стандартная библиотека)

### Взаимодействие между модулями

```
datasets_json_former.py
    ↓ создает
datasets_info.json + class_names.json
    ↓ использует
dataset_former.py
    ↓ создает
объединенный датасет + data.yaml
    ↓ использует
model_training_module.py
    ↓ использует
dataset_hash.py (вычисление хеша датасета)
    ↓ создает
обученные модели + метаданные с хешем
    ↓ координирует
training_queue.py
```

### Форматы данных на границах модулей

**datasets_info.json** (выход datasets_json_former, вход dataset_former):
```json
{
    "dataset_name": {
        "classes": {"class": index},
        "structure": "split|flat|nested_split|darknet",
        "elements_count": number_or_array
    }
}
```

**class_names.json** (выход datasets_json_former, вход dataset_former):
```json
{
    "original": "normalized"
}
```

**data.yaml** (выход dataset_former, вход model_training_module):
```yaml
train: ./train/images
val: ./valid/images
test: ./test/images
nc: 3
names: ['class1', 'class2', 'class3']
```

---

## Расширяемость и рекомендации

### Возможные улучшения

1. **Параллельная обработка**:
   - Добавить multiprocessing для обработки нескольких датасетов одновременно
   - Использовать `concurrent.futures` для параллельного копирования файлов

2. **Улучшенная система очереди**:
   - Добавить приоритеты задач
   - Реализовать перезапуск при ошибках
   - Добавить поддержку других терминалов (не только GNOME)

3. **Валидация данных**:
   - Проверка корректности аннотаций (координаты в диапазоне 0-1)
   - Проверка соответствия изображений и аннотаций
   - Валидация формата bounding box

4. **Кэширование**:
   - Кэширование результатов `detect_structure()`
   - Кэширование метаданных датасетов

5. **Логирование**:
   - Добавить модуль `logging` вместо `print()`
   - Сохранение логов в файлы
   - Разные уровни логирования (DEBUG, INFO, WARNING, ERROR)

6. **Конфигурационные файлы**:
   - Вынести константы в отдельный конфигурационный файл
   - Поддержка YAML/JSON конфигураций

### Рекомендации по модификации

**Добавление нового формата датасета**:
1. Добавить проверку в `detect_structure()`
2. Добавить обработку в `find_dataset_paths()`
3. Добавить подсчет в `count_elements()`
4. Обновить документацию

**Добавление новых метрик**:
1. Расширить `save_metrics_csv()` для сохранения дополнительных метрик
2. Использовать API Ultralytics для получения расширенных метрик

**Улучшение производительности**:
1. Использовать `pathlib` вместо `os.path` для работы с путями
2. Использовать генераторы вместо списков для больших датасетов
3. Добавить батчинг для операций с файлами

### Тестирование

Рекомендуемые тесты:
1. **Unit тесты** для каждой функции
2. **Integration тесты** для проверки взаимодействия модулей
3. **End-to-end тесты** для полного цикла работы
4. **Тесты на различных форматах** датасетов

Пример структуры тестов:
```
tests/
├── test_datasets_json_former.py
├── test_dataset_former.py
├── test_model_training_module.py
├── test_training_queue.py
└── fixtures/
    └── sample_datasets/
```

---

## Технические детали

### Производительность

**datasets_json_former.py**:
- Время выполнения: O(n × m), где n - количество датасетов, m - среднее количество файлов
- Память: O(n) для хранения метаданных

**dataset_former.py**:
- Время выполнения: O(n × m), где n - количество файлов, m - среднее количество объектов в аннотации
- Память: O(k) для хранения пар изображение-аннотация, где k - размер batch

**model_training_module.py**:
- Время выполнения: Зависит от Ultralytics (обычно часы для обучения)
- Память: Зависит от размера модели и batch size

**training_queue.py**:
- Время выполнения: O(1) для каждой операции (чтение, запись)
- Память: O(n) для хранения статусов, где n - количество задач

### Ограничения

1. **Размер датасетов**: Ограничен доступной памятью при загрузке всех пар в память
2. **Количество классов**: Теоретически неограничено, практически ограничено производительностью
3. **Длина путей**: Зависит от файловой системы (обычно 255-4096 символов)
4. **Размер файлов**: Ограничен файловой системой

### Безопасность

1. **Валидация путей**: Проверка существования перед операциями
2. **Обработка специальных символов**: Экранирование в командах shell
3. **Права доступа**: Использование стандартных функций Python для работы с файлами

---

## Заключение

Данная документация описывает внутреннюю реализацию системы обработки датасетов и обучения моделей YOLO. При модификации кода рекомендуется:

1. Следовать существующим паттернам обработки ошибок
2. Поддерживать обратную совместимость с существующими форматами данных
3. Добавлять валидацию входных данных
4. Обновлять документацию при изменении API
5. Тестировать изменения на различных форматах датасетов

Для получения дополнительной информации обратитесь к другим файлам документации в папке `docs/`.

